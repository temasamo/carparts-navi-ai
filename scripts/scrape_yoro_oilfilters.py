#!/usr/bin/env python3
"""
YORO STORE ã‚ªã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿè¡Œ: python scripts/scrape_yoro_oilfilters.py
å‡ºåŠ›: app/data/fitment_oilfilters.json
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin

BASE_URL = "https://www.yoro-store.com"
# ã‚ªã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ï¼ˆå®Ÿéš›ã®URLã«è¦èª¿æ•´ï¼‰
CATEGORY_URL = f"{BASE_URL}/product-category/oil-filter/"

def scrape_products():
    """ã‚ªã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å•†å“ä¸€è¦§ã‚’å–å¾—"""
    print(f"ğŸ“¦ ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {CATEGORY_URL}")
    res = requests.get(CATEGORY_URL, headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    res.raise_for_status()
    
    soup = BeautifulSoup(res.text, "html.parser")
    items = []
    
    # å•†å“ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«åˆã‚ã›ã¦è¦èª¿æ•´ï¼‰
    product_links = soup.select(".product-item a, .product-title a, .woocommerce-loop-product__link")
    
    if not product_links:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‹ã‚‰å•†å“ãƒšãƒ¼ã‚¸ã‚’æ¢ã™
        all_links = soup.select("a[href*='pid='], a[href*='product']")
        product_links = all_links[:50]  # æœ€åˆã®50ä»¶ã‚’å–å¾—
    
    print(f"ğŸ” {len(product_links)}ä»¶ã®å•†å“ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
    
    for idx, link in enumerate(product_links, 1):
        href = link.get("href", "")
        if not href:
            continue
        
        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
        if href.startswith("/"):
            url = urljoin(BASE_URL, href)
        elif not href.startswith("http"):
            url = urljoin(BASE_URL, href)
        else:
            url = href
        
        print(f"  [{idx}/{len(product_links)}] å‡¦ç†ä¸­: {url}")
        
        try:
            product = scrape_product_detail(url)
            if product:
                items.append(product)
                print(f"    âœ… å–å¾—æˆåŠŸ: {product['product_name'][:50]}...")
            else:
                print(f"    âš ï¸  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸ååˆ†ï¼‰")
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦åˆ¶é™ï¼ˆ2-3ç§’é–“éš”ï¼‰
        time.sleep(2.5)
    
    return items

def scrape_product_detail(url):
    """å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—"""
    res = requests.get(url, headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    res.raise_for_status()
    
    soup = BeautifulSoup(res.text, "html.parser")
    
    # å•†å“åã‚’å–å¾—
    name_elem = soup.select_one("h1, .product-title, .entry-title")
    if not name_elem:
        return None
    product_name = name_elem.text.strip()
    
    # ä¾¡æ ¼ã‚’å–å¾—
    price_elem = soup.select_one(".price, .woocommerce-Price-amount, .product-price")
    price = price_elem.text.strip() if price_elem else "ä¾¡æ ¼ä¸æ˜"
    
    # é©åˆè»Šç¨®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
    fitments = []
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰é©åˆæƒ…å ±ã‚’æŠ½å‡º
    tables = soup.select("table, .fitment-table, .compatibility-table")
    for table in tables:
        rows = table.select("tr")
        for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            cols = [c.text.strip() for c in row.select("td, th")]
            if len(cols) >= 4:
                fitments.append({
                    "maker": cols[0] if len(cols) > 0 else "",
                    "model": cols[1] if len(cols) > 1 else "",
                    "engine": cols[2] if len(cols) > 2 else "",
                    "year_range": cols[3] if len(cols) > 3 else ""
                })
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒªã‚¹ãƒˆå½¢å¼ã‚’è©¦ã™
    if not fitments:
        fitment_lists = soup.select(".fitment-list, .compatibility-list, ul.fitment")
        for fitment_list in fitment_lists:
            items = fitment_list.select("li")
            for item in items:
                text = item.text.strip()
                # ã€Œãƒ¡ãƒ¼ã‚«ãƒ¼ è»Šç¨® ã‚¨ãƒ³ã‚¸ãƒ³ å¹´å¼ã€å½¢å¼ã‚’æƒ³å®š
                parts = text.split()
                if len(parts) >= 4:
                    fitments.append({
                        "maker": parts[0],
                        "model": parts[1],
                        "engine": parts[2],
                        "year_range": parts[3]
                    })
    
    return {
        "product_name": product_name,
        "price": price,
        "url": url,
        "fitments": fitments
    }

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ YORO STORE ã‚ªã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹\n")
    
    try:
        data = scrape_products()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª
        output_dir = "data"
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, "fitment_oilfilters.json")
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å®Œäº†: {len(data)}ä»¶ã®å•†å“ã‚’ {output_path} ã«å‡ºåŠ›ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise

if __name__ == "__main__":
    main()
